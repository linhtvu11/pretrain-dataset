# Configuration file for LLM Pretraining Datasets
# VERIFIED: All datasets have been checked and confirmed to exist on HuggingFace (Nov 2025)
# This file lists datasets suitable for pretraining LLMs with focus on code, tech, science, bio, and QA

english_datasets:
  # Large-scale web and general text datasets
  - name: "HuggingFaceFW/fineweb"
    description: "High-quality web dataset, 15T tokens, deduplicated and filtered"
    text_field: "text"
    split: "train"
    streaming: true
    categories: ["general", "tech"]
    verified: true

  - name: "allenai/c4"
    description: "Colossal Clean Crawled Corpus - 750GB of cleaned web text"
    text_field: "text"
    config: "en"  # Specify English language
    split: "train"
    streaming: true
    categories: ["general"]
    verified: true

  - name: "Skylion007/openwebtext"
    description: "Open source recreation of WebText dataset from Reddit links"
    text_field: "text"
    split: "train"
    streaming: true
    categories: ["general", "qa"]
    verified: true

  - name: "wikimedia/wikipedia"
    description: "Wikipedia dump - English version (Nov 2023)"
    text_field: "text"
    config: "20231101.en"  # English Wikipedia from Nov 1, 2023
    split: "train"
    streaming: true
    categories: ["general", "knowledge"]
    verified: true

  # Code datasets
  - name: "bigcode/the-stack"
    description: "6TB of permissively-licensed source code, 358 programming languages"
    text_field: "content"
    split: "train"
    streaming: true
    categories: ["code"]
    verified: true

  - name: "codeparrot/github-code"
    description: "115M code files from GitHub in 32 programming languages"
    text_field: "code"
    split: "train"
    streaming: true
    categories: ["code"]
    verified: true

  # Science and academic datasets
  - name: "armanc/scientific_papers"
    description: "Scientific papers from ArXiv and PubMed"
    text_field: "article"  # Also has "abstract" field
    config: "arxiv"  # Can also use "pubmed"
    split: "train"
    streaming: true
    categories: ["science", "bio"]
    verified: true

  # Question Answering datasets
  - name: "rajpurkar/squad"
    description: "Stanford Question Answering Dataset, 100K+ QA pairs"
    text_field: "context"  # Also has "question" and "answers"
    split: "train"
    streaming: false
    categories: ["qa"]
    verified: true

  - name: "allenai/cosmos_qa"
    description: "35.6K commonsense-based reading comprehension problems"
    text_field: "context"  # Also has "question" and "answer0-3"
    split: "train"
    streaming: false
    categories: ["qa"]
    verified: true

  - name: "microsoft/wiki_qa"
    description: "Wikipedia-based question answering dataset"
    text_field: "answer"  # Also has "question" field
    split: "train"
    streaming: false
    categories: ["qa", "general"]
    verified: true

vietnamese_datasets:
  # Large-scale Vietnamese pretraining datasets
  - name: "vietgpt/binhvq_news_vi"
    description: "19.4M Vietnamese news articles, 4.78GB"
    text_field: "text"
    split: "train"
    streaming: true
    categories: ["general", "news"]
    verified: true

  - name: "bkai-foundation-models/NewsCategory"
    description: "596K categorized Vietnamese news articles from VnExpress"
    text_field: "content"  # Also has "title" and "sapo" (summary)
    split: "train"
    streaming: true
    categories: ["general", "news"]
    verified: true

  - name: "wikimedia/wikipedia"
    description: "Vietnamese Wikipedia (Nov 2023)"
    text_field: "text"
    config: "20231101.vi"  # Vietnamese Wikipedia from Nov 1, 2023
    split: "train"
    streaming: true
    categories: ["general", "knowledge"]
    verified: true

  - name: "opendatalab/WanJuan-Vietnamese"
    description: "280GB+ Vietnamese corpus with 7 major categories"
    text_field: "content"  # Also has "title" field
    split: "train"
    streaming: true
    categories: ["general", "knowledge"]
    verified: true

  - name: "uitnlp/vietnamese_students_feedback"
    description: "16K+ Vietnamese sentences with sentiment and topic annotations"
    text_field: "sentence"
    split: "train"
    streaming: false
    categories: ["qa", "feedback"]
    verified: true

  - name: "bkai-foundation-models/vi-alpaca"
    description: "50K Vietnamese instruction-following examples"
    text_field: "output"  # Also has "instruction" and "input" fields
    split: "train"
    streaming: false
    categories: ["qa", "instructions"]
    verified: true

# Filtering configuration
filtering:
  # Minimum and maximum text length (in characters)
  min_length: 100
  max_length: 1000000

  # Keywords to KEEP (content related to these will be prioritized)
  keep_keywords:
    # English keywords
    - "code"
    - "programming"
    - "algorithm"
    - "function"
    - "class"
    - "method"
    - "science"
    - "research"
    - "study"
    - "biology"
    - "chemistry"
    - "physics"
    - "mathematics"
    - "technology"
    - "engineering"
    - "computer"
    - "software"
    - "hardware"
    - "data"
    - "analysis"
    - "question"
    - "answer"
    - "solution"
    - "problem"
    - "explanation"
    - "tutorial"
    - "documentation"
    - "development"
    - "machine learning"
    - "artificial intelligence"
    - "neural network"
    - "database"
    - "api"

    # Vietnamese keywords - Tech/Code
    - "lập trình"
    - "chương trình"
    - "phần mềm"
    - "ứng dụng"
    - "phát triển"
    - "code"
    - "thuật toán"
    - "hàm"
    - "biến"
    - "mảng"
    - "cơ sở dữ liệu"
    - "máy tính"
    - "tính toán"

    # Vietnamese keywords - Science
    - "khoa học"
    - "nghiên cứu"
    - "học thuật"
    - "sinh học"
    - "hóa học"
    - "vật lý"
    - "toán học"
    - "phân tích"
    - "thí nghiệm"
    - "nghiên cứu khoa học"

    # Vietnamese keywords - Technology
    - "công nghệ"
    - "công nghệ thông tin"
    - "kỹ thuật"
    - "phần cứng"
    - "dữ liệu"
    - "trí tuệ nhân tạo"
    - "học máy"
    - "mạng nơ-ron"
    - "big data"

    # Vietnamese keywords - QA/Education
    - "câu hỏi"
    - "câu trả lời"
    - "giải pháp"
    - "vấn đề"
    - "giải thích"
    - "hướng dẫn"
    - "học tập"
    - "giáo dục"
    - "kiến thức"
    - "tài liệu"

  # Keywords to EXCLUDE (junk/spam indicators)
  exclude_keywords:
    # English spam/junk
    - "subscribe now"
    - "click here"
    - "buy now"
    - "limited offer"
    - "spam"
    - "advertisement"
    - "casino"
    - "viagra"
    - "lottery"
    - "winner"
    - "claim your prize"
    - "act now"
    - "free money"
    - "congratulations"
    - "you won"
    - "click below"
    - "special promotion"
    - "limited time"
    - "call now"
    - "earn money fast"
    - "make money online"
    - "work from home"
    - "get rich quick"
    - "lose weight fast"
    - "miracle cure"

    # Vietnamese spam/junk
    - "đăng ký ngay"
    - "nhấp vào đây"
    - "mua ngay"
    - "ưu đãi có hạn"
    - "giảm giá sốc"
    - "khuyến mãi"
    - "quảng cáo"
    - "casino"
    - "cá độ"
    - "cờ bạc"
    - "xổ số"
    - "trúng thưởng"
    - "nhận quà"
    - "miễn phí"
    - "kiếm tiền nhanh"
    - "làm giàu"
    - "thu nhập cao"
    - "làm việc tại nhà"
    - "bí quyết"
    - "thần kỳ"
    - "đặc biệt"
    - "hàng giả"
    - "hàng nhái"
    - "gọi ngay"
    - "nhấc máy"
    - "tin nhắn rác"
    - "spam"
    - "lừa đảo"
    - "chiêu trò"
    - "mạo danh"

  # Exclude texts with high ratio of these patterns
  junk_patterns:
    - "!!!!!+"  # Excessive exclamation marks
    - "\\$\\$\\$+"  # Excessive dollar signs
    - "https?://[^\\s]{200,}"  # Very long URLs
    - "\\*{5,}"  # Excessive asterisks
    - "#{5,}"  # Excessive hash symbols
    - "={5,}"  # Excessive equal signs
    - "[!?]{3,}"  # Multiple punctuation (!!!, ???, etc.)
    - "(?i)(buy|click|subscribe|register).*now.*!!+"  # Spam call-to-action patterns
    - "(?i)(mua|đăng\\s*ký|nhấp|gọi).*ngay.*[!]{2,}"  # Vietnamese spam patterns

  # Language detection threshold (0-1)
  language_confidence_threshold: 0.7

  # Deduplication settings
  deduplication:
    enabled: true
    similarity_threshold: 0.85  # Cosine similarity threshold

# Output configuration
output:
  dataset_name: "merged_llm_pretrain_dataset"
  push_to_hub: true
  hub_repo: "your-username/merged-llm-pretrain"  # Update with your HuggingFace username
  private: false
  max_samples_per_dataset: null  # null means no limit, or set a number
