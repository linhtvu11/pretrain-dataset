# Configuration file for LLM Pretraining Datasets
# VERIFIED: All datasets have been checked and confirmed to exist on HuggingFace (Nov 2025)
# This file lists datasets suitable for pretraining LLMs with focus on code, tech, science, bio, and QA

english_datasets:
  # Large-scale web and general text datasets
  - name: "HuggingFaceFW/fineweb"
    description: "High-quality web dataset, 15T tokens, deduplicated and filtered"
    text_field: "text"
    split: "train"
    streaming: true
    categories: ["general", "tech"]
    verified: true

  - name: "allenai/c4"
    description: "Colossal Clean Crawled Corpus - 750GB of cleaned web text"
    text_field: "text"
    config: "en"  # Specify English language
    split: "train"
    streaming: true
    categories: ["general"]
    verified: true

  - name: "Skylion007/openwebtext"
    description: "Open source recreation of WebText dataset from Reddit links"
    text_field: "text"
    split: "train"
    streaming: true
    categories: ["general", "qa"]
    verified: true

  - name: "wikimedia/wikipedia"
    description: "Wikipedia dump - English version (Nov 2023)"
    text_field: "text"
    config: "20231101.en"  # English Wikipedia from Nov 1, 2023
    split: "train"
    streaming: true
    categories: ["general", "knowledge"]
    verified: true

  # Code datasets
  - name: "bigcode/the-stack"
    description: "6TB of permissively-licensed source code, 358 programming languages"
    text_field: "content"
    split: "train"
    streaming: true
    categories: ["code"]
    verified: true

  - name: "codeparrot/github-code"
    description: "115M code files from GitHub in 32 programming languages"
    text_field: "code"
    split: "train"
    streaming: true
    categories: ["code"]
    verified: true

  # Science and academic datasets
  - name: "armanc/scientific_papers"
    description: "Scientific papers from ArXiv and PubMed"
    text_field: "article"  # Also has "abstract" field
    config: "arxiv"  # Can also use "pubmed"
    split: "train"
    streaming: true
    categories: ["science", "bio"]
    verified: true

  # Question Answering datasets
  - name: "rajpurkar/squad"
    description: "Stanford Question Answering Dataset, 100K+ QA pairs"
    text_field: "context"  # Also has "question" and "answers"
    split: "train"
    streaming: false
    categories: ["qa"]
    verified: true

  - name: "allenai/cosmos_qa"
    description: "35.6K commonsense-based reading comprehension problems"
    text_field: "context"  # Also has "question" and "answer0-3"
    split: "train"
    streaming: false
    categories: ["qa"]
    verified: true

  - name: "microsoft/wiki_qa"
    description: "Wikipedia-based question answering dataset"
    text_field: "answer"  # Also has "question" field
    split: "train"
    streaming: false
    categories: ["qa", "general"]
    verified: true

vietnamese_datasets:
  # Large-scale Vietnamese pretraining datasets
  - name: "vietgpt/binhvq_news_vi"
    description: "19.4M Vietnamese news articles, 4.78GB"
    text_field: "text"
    split: "train"
    streaming: true
    categories: ["general", "news"]
    verified: true

  - name: "bkai-foundation-models/NewsCategory"
    description: "596K categorized Vietnamese news articles from VnExpress"
    text_field: "content"  # Also has "title" and "sapo" (summary)
    split: "train"
    streaming: true
    categories: ["general", "news"]
    verified: true

  - name: "wikimedia/wikipedia"
    description: "Vietnamese Wikipedia (Nov 2023)"
    text_field: "text"
    config: "20231101.vi"  # Vietnamese Wikipedia from Nov 1, 2023
    split: "train"
    streaming: true
    categories: ["general", "knowledge"]
    verified: true

  - name: "opendatalab/WanJuan-Vietnamese"
    description: "280GB+ Vietnamese corpus with 7 major categories"
    text_field: "content"  # Also has "title" field
    split: "train"
    streaming: true
    categories: ["general", "knowledge"]
    verified: true

  - name: "uitnlp/vietnamese_students_feedback"
    description: "16K+ Vietnamese sentences with sentiment and topic annotations"
    text_field: "sentence"
    split: "train"
    streaming: false
    categories: ["qa", "feedback"]
    verified: true

  - name: "bkai-foundation-models/vi-alpaca"
    description: "50K Vietnamese instruction-following examples"
    text_field: "output"  # Also has "instruction" and "input" fields
    split: "train"
    streaming: false
    categories: ["qa", "instructions"]
    verified: true

# Filtering configuration
filtering:
  # Minimum and maximum text length (in characters)
  min_length: 100
  max_length: 1000000

  # Keywords to KEEP (content related to these will be prioritized)
  keep_keywords:
    - "code"
    - "programming"
    - "algorithm"
    - "function"
    - "class"
    - "method"
    - "science"
    - "research"
    - "study"
    - "biology"
    - "chemistry"
    - "physics"
    - "mathematics"
    - "technology"
    - "engineering"
    - "computer"
    - "software"
    - "hardware"
    - "data"
    - "analysis"
    - "question"
    - "answer"
    - "solution"
    - "problem"
    - "explanation"
    # Vietnamese keywords
    - "lập trình"
    - "khoa học"
    - "công nghệ"
    - "dữ liệu"
    - "thuật toán"

  # Keywords to EXCLUDE (junk/spam indicators)
  exclude_keywords:
    - "subscribe now"
    - "click here"
    - "buy now"
    - "limited offer"
    - "spam"
    - "advertisement"
    - "casino"
    - "viagra"
    - "lottery"
    - "winner"
    - "claim your prize"

  # Exclude texts with high ratio of these patterns
  junk_patterns:
    - "!!!!!+"  # Excessive exclamation
    - "\\$\\$\\$+"  # Excessive dollar signs
    - "https?://[^\\s]{200,}"  # Very long URLs

  # Language detection threshold (0-1)
  language_confidence_threshold: 0.7

  # Deduplication settings
  deduplication:
    enabled: true
    similarity_threshold: 0.85  # Cosine similarity threshold

# Output configuration
output:
  dataset_name: "merged_llm_pretrain_dataset"
  push_to_hub: true
  hub_repo: "your-username/merged-llm-pretrain"  # Update with your HuggingFace username
  private: false
  max_samples_per_dataset: null  # null means no limit, or set a number
